{
  
    
        "post0": {
            "title": "Multi-Task Gaussian Processes for Vital Sign Imputation",
            "content": "This is an extension of my previous blog post on gaussian processes. In this post we will cover how to use GPytorch to model multiple vital signs at the same time. One would want to do this when you have multiple features that are correlated or have obvious dependencies. Another of looking at would be if you have a set of features where the missing values are Missing at Random. This is an alternative to using a single gaussian process model to impute each feature independently. Other alternatives include linear interpolation, K-nearest-neighbors, Bayesian Ridge Regression, Random Forest, etc. More can be found here from sci-kit-learn. . We&#39;ll be using public data from the Physionet 2019 competition on the early prediction of sepsis. Data can be found here . Sources . The original paper for Multi-task Gaussian processes. . This application for using MTGP for vital signs is based on the approach from this paper . Most of the GPyTorch code comes from this example. . Data . Here is a short code snippet to prepare the data: . #collapse import pandas as pd import os import numpy as np import re from cache_em_all import Cachable DATA_DIR = &quot;training_setB/&quot; # Path to the data def load_single_file(file_path): df = pd.read_csv(file_path, sep=&#39;|&#39;) df[&#39;hours&#39;] = df.index df[&#39;patient&#39;] = re.search(&#39;p(.*?).psv&#39;, file_path).group(1) return df def get_data_files(): return [os.path.join(DATA_DIR, x) for x in sorted(os.listdir(DATA_DIR)) if int(x[1:-4]) % 5 &gt; 0] @Cachable(&#39;training_setB.csv&#39;) def load_data(): data = get_data_files() data_frames = [load_single_file(d) for d in data] merged = pd.concat(data_frames) return merged . . We&#39;ll be looking patient 10019, who has a good amount of missing data. We&#39;ll only be modeling 6 vital signs: . Heart rate (HR) | Temperature (Temp) | Respiratory Rate (Resp) | Systolic Blood Pressure (SBP) | Diastolic blood Pressure (DBP) | Mean Arteiral Pressure (MAP) | . df = load_data() df = df.loc[df.patient == 100019] [[&#39;HR&#39;, &#39;Temp&#39;, &#39;SBP&#39;, &#39;DBP&#39;, &#39;MAP&#39;, &#39;Resp&#39;]].plot().legend(loc=&#39;center left&#39;, bbox_to_anchor=(1.0, 0.5)) . The goal will be to model these 6 features simultaneously to fill in the missing values. . Defining our Model . We&#39;ll be using the GPyTorch package for our guassian processes, the only package I&#39;m aware of in Python that supports multi-task learning for gaussian processes. . Our model will consist of a python class with a constructor and a forward method that is called in our training loop later. . What&#39;s important to note here is we must specify how many tasks we want to learn. In our case, that is 6. Another difference from the Exact GP model is that we are defining two covariance modules, the standard RBFKernal, and the index kernal, which according to the documentation &quot;is a lookup table containing inter-task covariance&quot;. . Our final covariance kernal is a multiplication of these two covariance kernals: $k([x,i], [x&#39;, j]) = k_{inputs}(x, x&#39;) * k_{tasks}(i,j)$ . In our code: covar = covar_x.mul(covar_i) . class MultitaskGPModel(gpytorch.models.ExactGP): def __init__(self, train_x, train_y, likelihood): super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood) self.mean_module = gpytorch.means.ConstantMean() self.covar_module = gpytorch.kernels.RBFKernel() self.task_covar_module = gpytorch.kernels.IndexKernel(num_tasks=6, rank=1) def forward(self,x,i): mean_x = self.mean_module(x) covar_x = self.covar_module(x) covar_i = self.task_covar_module(i) covar = covar_x.mul(covar_i) return gpytorch.distributions.MultivariateNormal(mean_x, covar) . Data Preparation . This is the most tricky part. . I&#39;ll explain what to contitutes full_train_x, full_train_i, and full_train_y in our instatiation of our Multitask model: . MultitaskGPModel((full_train_x, full_train_i), full_train_y, likelihood) . df = load_data() df = df.loc[df.patient == 100019] . Assuming that we will repeat this process with many patients, there inevitably will be a patient for which an entire feature is missing. We will want to exclude that, and impute with the average or median for that feature after. . impute_features = [&#39;HR&#39;, &#39;DBP&#39;, &#39;Temp&#39;, &#39;SBP&#39;, &#39;MAP&#39;, &#39;Resp&#39;] missing_feats = df.columns[df.isna().all()].tolist() non_empty_feats = [f for f in set(impute_features) - set(missing_feats)] non_empty_feats . [&#39;HR&#39;, &#39;Resp&#39;, &#39;SBP&#39;, &#39;DBP&#39;, &#39;MAP&#39;, &#39;Temp&#39;] . We than collect each of our features into a list of features that are torch tensors, our first tensor in the list being temperature: . feats = [] for f in non_empty_feats: feats.append(torch.tensor(df[f].values, dtype=torch.float)) feats[0] . tensor([nan, 56., 58., 56., 54., 52., 52., 58., 54., 54., 52., 58., 54., 58., 58., 58., 56., 50., 54., 52., 52., 52., 54., 58., 58., 70., 68., 68., 64., nan, 64., nan, 61., nan, 56., 58., 62., 58., 62., nan, 58., 60., 60., 62., 68., nan]) . We define our full_train_x as the indicies with non-null values. Here, we see that our temperature feature has non-null values at index 3, 12, 14, etc. . full_train_x = [torch.where(~torch.isnan(f))[0] for f in feats] full_train_x[0] . tensor([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 32, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44]) . We then concatenate all the features index tensors together into one: . full_train_x = torch.cat(full_train_x) full_train_x . tensor([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 32, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 37, 38, 40, 41, 42, 43, 44, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 32, 34, 35, 37, 38, 40, 41, 42, 43, 44, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 32, 34, 35, 37, 38, 40, 41, 42, 43, 44, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 32, 34, 35, 37, 38, 40, 41, 42, 43, 44, 3, 12, 14, 18, 20, 22, 34, 37, 42]) . For full_train_i, this is the concatenated tensor of indicies to specify which value belongs to which feature, as if we had given an ordinal label to each feature. . In this example, we have assigned 0 to the temperature feature, and there is 9 non-null temperature values in our observation. We assign 1 to DBP, 2 to Resp, etc. . full_train_i = [] for i, f in enumerate(feats): full_train_i.append(torch.full_like(f[~torch.isnan(f)], dtype=torch.long, fill_value=i)) full_train_i = torch.cat(full_train_i) full_train_i . tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5]) . Finally, full_train_y are the non-null observed values for each feature, concatenated together. . full_train_y = [f[~torch.isnan(f)] for f in feats] full_train_y = torch.cat(full_train_y) full_train_y . tensor([ 56.0000, 58.0000, 56.0000, 54.0000, 52.0000, 52.0000, 58.0000, 54.0000, 54.0000, 52.0000, 58.0000, 54.0000, 58.0000, 58.0000, 58.0000, 56.0000, 50.0000, 54.0000, 52.0000, 52.0000, 52.0000, 54.0000, 58.0000, 58.0000, 70.0000, 68.0000, 68.0000, 64.0000, 64.0000, 61.0000, 56.0000, 58.0000, 62.0000, 58.0000, 62.0000, 58.0000, 60.0000, 60.0000, 62.0000, 68.0000, 22.0000, 24.0000, 22.0000, 20.0000, 20.0000, 22.0000, 20.0000, 18.0000, 16.0000, 16.0000, 14.0000, 20.0000, 18.0000, 18.0000, 20.0000, 18.0000, 24.0000, 22.0000, 21.0000, 19.0000, 22.0000, 18.0000, 16.0000, 18.0000, 16.0000, 19.0000, 22.0000, 18.0000, 20.0000, 18.0000, 145.0000, 163.0000, 151.0000, 112.0000, 113.0000, 109.0000, 158.0000, 127.0000, 129.0000, 101.0000, 176.0000, 190.0000, 188.0000, 198.0000, 164.0000, 160.0000, 156.0000, 191.0000, 147.0000, 170.0000, 158.0000, 172.0000, 176.0000, 155.5000, 182.0000, 143.0000, 133.0000, 156.0000, 160.0000, 135.5000, 153.0000, 180.0000, 177.0000, 194.0000, 158.0000, 222.0000, 170.0000, 156.0000, 153.0000, 74.0000, 77.0000, 63.0000, 57.0000, 64.0000, 63.0000, 77.0000, 63.0000, 65.0000, 63.0000, 82.0000, 105.0000, 92.0000, 85.0000, 74.0000, 71.0000, 65.0000, 86.0000, 68.0000, 68.0000, 71.0000, 85.0000, 119.0000, 80.0000, 85.0000, 72.0000, 69.0000, 87.0000, 78.0000, 76.5000, 71.0000, 78.0000, 81.0000, 77.0000, 71.5000, 93.0000, 76.0000, 69.0000, 72.0000, 103.0000, 116.0000, 105.0000, 79.0000, 84.0000, 95.0000, 112.0000, 86.0000, 92.0000, 84.0000, 100.0000, 147.0000, 146.0000, 142.0000, 99.0000, 97.0000, 101.0000, 140.0000, 91.0000, 95.0000, 118.0000, 116.0000, 152.0000, 116.0000, 134.0000, 92.0000, 98.0000, 137.0000, 118.0000, 101.5000, 98.0000, 102.0000, 113.5000, 110.0000, 92.0000, 142.0000, 105.0000, 100.0000, 101.0000, 36.8000, 36.4000, 36.5000, 36.4000, 36.0000, 36.5000, 36.7000, 36.6000, 36.5000]) . Important: Double check to make sure your features are in the same order for each of full_train_x, full_train_i, and full_train_y . Now that we have all our training data is the correct format, we just need to instantiate our likelihood. Note that our train_x argument for the MultitaskGPModel must be a tuple of (full_train_x, full_train_i). . likelihood = gpytorch.likelihoods.GaussianLikelihood() model = MultitaskGPModel((full_train_x, full_train_i), full_train_y, likelihood) . Training Loop . Now fit our model to the data. Our model input is the training tuple (full_train_x, full_train_i), which we then feed into the ExactMarginalLogLikelihood MLE to determine our loss, by comparison with full_train_y. 10,000 iterations seems to work for a good fit visually. . training_iterations = 10_000 model.train() likelihood.train() optimizer = torch.optim.Adam([ {&#39;params&#39;: model.parameters()}, ], lr=0.1) mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model) for i in range(training_iterations): optimizer.zero_grad() output = model(full_train_x, full_train_i) loss = -mll(output, full_train_y) loss.backward() optimizer.step() . Model Prediction . Now we place our model and likelihood in evaluation mode, and create test_x, a tensor that holds all full list (non null and null) of indicies, and test_i_tasks, which contains a list of tensors with indicies that indentify our features with full length, which means they are the length of features with non and null values. . model.eval() likelihood.eval() timesteps = len(df) test_x = torch.tensor(list(range(timesteps)), dtype=torch.long) test_x . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]) . test_i_tasks = [] for i in range(len(non_empty_feats)): test_i_tasks.append(torch.full_like(test_x, dtype=torch.long, fill_value=i)) test_i_tasks . [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]), tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]), tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])] . Finally, we create a list of MultivariateNormal models in observed_pred_ys, and we predict the mean of each of these models to impute our features: . observed_pred_ys = [] with torch.no_grad(), gpytorch.settings.fast_pred_samples(): for test_i_task in test_i_tasks: observed_pred_ys.append(likelihood(model(test_x, test_i_task))) for f, preds in zip(non_empty_feats, observed_pred_ys): df[f] = preds.mean.detach().numpy() df[impute_features].plot().legend(loc=&#39;center left&#39;, bbox_to_anchor=(1.0, 0.5)) . &lt;matplotlib.legend.Legend at 0x1219658d0&gt; . From our imputed graph we can easily compare and see that where there were previously missing values, we have no filled them in with reference to the other trends in vitals signs. . Going beyond 1 patient . We can put all our previous code in a function MTGP_impute(df, timesteps), which takes as input our df for a patient and the number of rows (timesteps) we have observed for that patient: . #collapse def MTGP_impute(df, timesteps): impute_features = [&#39;HR&#39;, &#39;DBP&#39;, &#39;Temp&#39;, &#39;SBP&#39;, &#39;MAP&#39;, &#39;Resp&#39;] df[impute_features].plot().legend(loc=&#39;center left&#39;, bbox_to_anchor=(1.0, 0.5)) missing_feats = df.columns[df.isna().all()].tolist() non_empty_feats = [f for f in set(impute_features) - set(missing_feats)] feats = [] for f in non_empty_feats: feats.append(torch.tensor(df[f].values, dtype=torch.float)) full_train_x = [torch.where(~torch.isnan(f))[0] for f in feats] if full_train_x == []: return df full_train_x = torch.cat(full_train_x) full_train_i = [] for i, f in enumerate(feats): full_train_i.append(torch.full_like(f[~torch.isnan(f)], dtype=torch.long, fill_value=i)) full_train_i = torch.cat(full_train_i) full_train_y = [f[~torch.isnan(f)] for f in feats] full_train_y = torch.cat(full_train_y) # Instantiate likelihood and model likelihood = gpytorch.likelihoods.GaussianLikelihood() model = MultitaskGPModel((full_train_x, full_train_i), full_train_y, likelihood) training_iterations = 10000 model.train() likelihood.train() optimizer = torch.optim.Adam([ {&#39;params&#39;: model.parameters()}, # Includes GaussianLikelihood parameters ], lr=0.1) mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model) for i in range(training_iterations): optimizer.zero_grad() output = model(full_train_x, full_train_i) loss = -mll(output, full_train_y) loss.backward() optimizer.step() model.eval() likelihood.eval() test_x = torch.tensor(list(range(timesteps)), dtype=torch.long) test_i_tasks = [] for i in range(len(non_empty_feats)): test_i_tasks.append(torch.full_like(test_x, dtype=torch.long, fill_value=i)) observed_pred_ys = [] with torch.no_grad(), gpytorch.settings.fast_pred_samples(): for test_i_task in test_i_tasks: observed_pred_ys.append(likelihood(model(test_x, test_i_task))) for f, preds in zip(non_empty_feats, observed_pred_ys): df[f] = preds.mean.detach().numpy() df[impute_features].plot().legend(loc=&#39;center left&#39;, bbox_to_anchor=(1.0, 0.5)) return df . . df = load_data() df = df.loc[df.patient == 100019] . MTGP_impute(df, len(df)) . We can use our function to iterate through each patient in our full data frame. This is the second data set prodicded by Physionet, and contains 16,000 patients: . df = load_data() impute_df = [MTGP_impute(df_group, len(df_group)) for patient, df_group in tqdm(df.groupby(&#39;patient&#39;))] impute_df = pd.concat(impute_df) . 0%| | 3/16000 [01:37&lt;141:45:35, 31.90s/it] . Looks like might take 141 hours, or 5-6 days. That will be unacceptable for most people. Enter: Joblib . We can parallelize our fucntion across multiple cores. My current machine has 8 cores: . impute_df = Parallel(n_jobs=8)(delayed(MTGP_impute)(df_group, len(df_group)) for patient, df_group in tqdm(df.groupby(&#39;patient&#39;))) . 0%| | 24/16000 [01:21&lt;6:56:02, 1.56s/it] . Now, its looks like we can finish our imputation in under 7 hours! Also, instead of doing 10,000 iterations for each patient, we could stop iterating when our loss only changes a small amount each iteration. . I hope you have found this useful, and please let me know of any mistakes or clarifications I can make! .",
            "url": "https://zs-barnes.github.io/blog/gaussian%20processes/2020/08/20/MTGP.html",
            "relUrl": "/gaussian%20processes/2020/08/20/MTGP.html",
            "date": " • Aug 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Modeling Vital signs with Gaussian Processes",
            "content": "As an alternative to linear regression, using a gaussian processes one can predict a distribution for each input, allowing for confidence levels instead of a hard prediction. Here, we use the GPyTorch library to model patient vital sign data from the 2019 Physionet Sepsis Competition: https://physionet.org/content/challenge-2019/1.0.0/ . Note: Much of this tutorial is from https://docs.gpytorch.ai/en/v1.1.1/examples/01_Exact_GPs/Simple_GP_Regression.html, but I wanted to document my own use case. This is not an introduction to guassian processes, but a tutorial on how to apply GPyTorch. . Motivation: A formidiable obstacle to modeling health data is that it is very sparse. The application in mind for using a guassian process to model the patient&#39;s vital signs are for imputation the of missing values. . We&#39;ll be using a random patient from the sepsis data provided by https://physionet.org/content/challenge-2019/1.0.0/ . . Data . Let&#39;s check out patient 57779: . df = pd.read_csv(&#39;p005779.psv&#39;, sep=&#39;|&#39;) . df.head() . HR O2Sat Temp SBP MAP DBP Resp EtCO2 BaseExcess HCO3 ... WBC Fibrinogen Platelets Age Gender Unit1 Unit2 HospAdmTime ICULOS SepsisLabel . 0 76.0 | 100.0 | 36.72 | 125.0 | 91.0 | 76.0 | 14.0 | NaN | NaN | NaN | ... | NaN | NaN | NaN | 33.11 | 1 | NaN | NaN | -0.03 | 3 | 0 | . 1 80.0 | 100.0 | NaN | 110.0 | 74.0 | 64.0 | 12.0 | NaN | NaN | NaN | ... | NaN | NaN | NaN | 33.11 | 1 | NaN | NaN | -0.03 | 4 | 0 | . 2 87.0 | 100.0 | 36.50 | 103.0 | 75.0 | 62.0 | 15.5 | NaN | -2.0 | 24.0 | ... | 13.6 | 245.0 | 156.0 | 33.11 | 1 | NaN | NaN | -0.03 | 5 | 0 | . 3 93.0 | 100.0 | NaN | 99.0 | 73.0 | 63.0 | 16.0 | NaN | -3.0 | NaN | ... | NaN | NaN | NaN | 33.11 | 1 | NaN | NaN | -0.03 | 6 | 0 | . 4 87.0 | 100.0 | 37.06 | 125.0 | 57.0 | 74.0 | 14.0 | NaN | NaN | NaN | ... | NaN | NaN | NaN | 33.11 | 1 | NaN | NaN | -0.03 | 7 | 0 | . 5 rows × 41 columns . df[[&#39;HR&#39;,&#39;O2Sat&#39;, &#39;Temp&#39;, &#39;SBP&#39;, &#39;MAP&#39;, &#39;DBP&#39;, &#39;Resp&#39;]].isnull().sum() . HR 1 O2Sat 1 Temp 23 SBP 1 MAP 1 DBP 24 Resp 1 dtype: int64 . Temperature is a feature that has a high number of missing values, so we&#39;ll use that for modeling. . Our data type for GPyTorch models are torch tensors. . temp = torch.tensor(df.Temp.values, dtype=torch.float32) . With missing values our training x and y points will be need to be chosen as: . x = index values where there is non null values | y = observed temperature values that are non null | . train_x = torch.where(~torch.isnan(temp))[0] train_x . tensor([ 0, 2, 4, 8, 12, 16, 20, 24, 28, 32]) . train_y = temp[~torch.isnan(temp)] train_y . tensor([36.7200, 36.5000, 37.0600, 37.0600, 36.8300, 37.0000, 37.6700, 37.7200, 37.2200, 37.0600]) . Model . Similiary to Pytorch, you must define a class for each model you construct. With GPyTorch, there are 5 objects you must specify: . GP model: gpytorch.models.ExactGP | Likelihood: gpytorch.likelihoods.GaussianLikelihood() | Mean: gpytorch.means.ConstantMean() | Kernal: gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) | Distribution: gpytorch.distributions.MultivariateNormal() | . These are all the &quot;default&quot; settings for a gaussian process. If you had a choice to tweak one, choose the kernal. A good place to learn more is at: https://www.cs.toronto.edu/~duvenaud/cookbook/ . class ExactGPModel(gpytorch.models.ExactGP): def __init__(self, train_x, train_y, likelihood): super(ExactGPModel, self).__init__(train_x, train_y, likelihood) self.mean_module = gpytorch.means.ConstantMean() self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) def forward(self, x): mean_x = self.mean_module(x) covar_x = self.covar_module(x) return gpytorch.distributions.MultivariateNormal(mean_x, covar_x) likelihood = gpytorch.likelihoods.GaussianLikelihood() model = ExactGPModel(train_x, train_y, likelihood) . Training loop . Similar to PyTorch models, you can train your model by specifying an optimizer and loss, then updating the gradients at each step. . training_iter = 10000 # Find optimal model hyperparameters model.train() likelihood.train() # Use the adam optimizer optimizer = torch.optim.Adam([ {&#39;params&#39;: model.parameters()}, # Includes GaussianLikelihood parameters ], lr=0.1) # &quot;Loss&quot; for GPs - the marginal log likelihood mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model) for i in range(training_iter): # Zero gradients from previous iteration optimizer.zero_grad() # Output from model output = model(train_x) # Calc loss and backprop gradients loss = -mll(output, train_y) loss.backward() if i % 1000 == 0: print(&#39;Iter %d/%d - Loss: %.3f lengthscale: %.3f noise: %.3f&#39; % ( i, training_iter, loss.item(), model.covar_module.base_kernel.lengthscale.item(), model.likelihood.noise.item() )) optimizer.step() . Iter 0/10000 - Loss: 495.607 lengthscale: 0.693 noise: 0.693 Iter 1000/10000 - Loss: 4.124 lengthscale: 13.291 noise: 3.872 Iter 2000/10000 - Loss: 1.933 lengthscale: 14.903 noise: 2.703 Iter 3000/10000 - Loss: 0.885 lengthscale: 16.011 noise: 0.128 Iter 4000/10000 - Loss: 0.793 lengthscale: 17.954 noise: 0.070 Iter 5000/10000 - Loss: 0.744 lengthscale: 20.278 noise: 0.073 Iter 6000/10000 - Loss: 0.673 lengthscale: 22.807 noise: 0.076 Iter 7000/10000 - Loss: 0.416 lengthscale: 23.878 noise: 0.089 Iter 8000/10000 - Loss: 0.386 lengthscale: 18.951 noise: 0.089 Iter 9000/10000 - Loss: 0.318 lengthscale: 8.548 noise: 0.071 . Now we get out predictions from the model: . # Get into evaluation (predictive posterior) mode model.eval() likelihood.eval() # Make predictions by feeding model through likelihood with torch.no_grad(), gpytorch.settings.fast_pred_var(): test_x = torch.tensor(list(df.index)) observed_pred = likelihood(model(test_x)) . Plots . The GPytorch tutorial provides matplotlib code for plotting the mean and confidence itervals of our posterior: . with torch.no_grad(): # Initialize plot f, ax = plt.subplots(1, 1, figsize=(8, 6)) # Get upper and lower confidence bounds lower, upper = observed_pred.confidence_region() # Plot training data as black stars ax.plot(train_x.numpy(), train_y.numpy(), &#39;k*&#39;) # Plot predictive means as blue line ax.plot(test_x.numpy(), observed_pred.mean.numpy(), &#39;b&#39;) # Shade between the lower and upper confidence bounds ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5) ax.legend([&#39;Observed Data&#39;, &#39;Mean&#39;, &#39;Confidence&#39;]) . But we can do better. PyMC3 has plotting functionality that allows us to sample from our posterior and plot the results. . from pymc3.gp.util import plot_gp_dist . gp_samples = torch.stack([observed_pred.sample() for i in range(100)]) . fig, ax = plt.subplots(figsize=(8,6)) plot_gp_dist(ax, gp_samples.numpy(), test_x.numpy()) df.reset_index().plot(kind=&#39;scatter&#39;, x=&#39;index&#39;, y=&#39;Temp&#39;, c=&#39;k&#39;, s=50, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff47c8b9710&gt; . With just 100 samples we get a clear picture of where the guassian process mean fits thorugh out data. While not just looking cool, graphing in this way helps to illustrate how the model is generated from a posterior distribution. .",
            "url": "https://zs-barnes.github.io/blog/gaussian%20processes/2020/08/12/SingleTaskGP.html",
            "relUrl": "/gaussian%20processes/2020/08/12/SingleTaskGP.html",
            "date": " • Aug 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Time Series Features in PySpark",
            "content": "This post comes from a place of frustration in not being able to create simple time series features with window functions like the median or slope in Pyspark. This approach is by no means optimal, but it got the job done for purposes. . from pyspark.sql import * from pyspark.sql.functions import * from pyspark.sql.types import * import sys import numpy as np import time . To make our work more organized, we use pyspark&#39;s ML pipeline tool. . from pyspark.ml.pipeline import Transformer from pyspark.ml import Pipeline . Our code all fits into one class, where we specify which feature to use, what size window, and what statistic we want to compute with each window. . Implementation . class HourWindowFeat(Transformer): def __init__(self, hours, feats, stat): self.hours = hours self.feats = feats self.stat = stat def this(): this(Identifiable.randomUID(&quot;HourWindowFeat&quot;)) def copy(extra): defaultCopy(extra) def slope(self, series): if series == [] or len(series) == 1: return 0 series = np.array(series) if (series == -1).all(): return 0 x = np.where(series != -1.0)[0] y = series[np.where(series != -1.0)] coefficients, residuals, _, _, _ = np.polyfit(x,y,1,full=True) return coefficients[0] def _transform(self, df): hour_to_sec = lambda i: i * 3600 w = (Window() .partitionBy(col(&quot;encounter_id&quot;)) .orderBy(col(&quot;time&quot;).cast(&#39;long&#39;)) .rangeBetween(-hour_to_sec(self.hours-1), 0)) if self.stat == &#39;median&#39;: median_udf = udf(lambda x: float(np.median(x)), FloatType()) for f in self.feats: output = str(self.hours) + &#39;_&#39; + &#39;hour&#39; + &#39;_&#39; + self.stat + &#39;_&#39; + f df = df.withColumn(&#39;list&#39;, collect_list(f).over(w)) .withColumn(output, round(median_udf(&#39;list&#39;), 2)) df = df.drop(&#39;list&#39;) elif self.stat == &#39;slope&#39;: slope_udf = udf(lambda x: float(self.slope(x)), FloatType()) for f in self.feats: output = str(self.hours) + &#39;_&#39; + &#39;hour&#39; + &#39;_&#39; + self.stat + &#39;_&#39; + f filled_column = &#39;na_filled_&#39; + f df = df.drop(&#39;list&#39;) df = df.withColumn(filled_column, df[f]).fillna({filled_column:-1}) .withColumn(&#39;list&#39;, collect_list(filled_column).over(w)) .withColumn(output, round(slope_udf(&#39;list&#39;), 2)) df = df.drop(&#39;list&#39;) df = df.drop(filled_column) else: for f in self.feats: output = str(self.hours) + &#39;_&#39; + &#39;hour&#39; + &#39;_&#39; + self.stat.__name__ + &#39;_&#39; + f df = df.withColumn(output, round(self.stat(f).over(w), 2)) return df . Real Use Case . Here is the code I used in my research to identify which features (vital signs, lab values) to use, which window sizes (24, 48, and 72 hours) and what statistics were computed on each(min, max, mean, median, stddev, slope). . feats_24 = [&#39;temperature&#39;,&#39;heart_rate&#39;,&#39;respiratory_rate&#39;,&#39;O2_saturation&#39;,&#39;systolic_blood_pressure&#39;, &#39;shock_index&#39;,&#39;diastolic_blood_pressure&#39;, &#39;pulse_pressure&#39;,&#39;mean_arterial_pressure&#39;,&#39;urine_output&#39;] feats_48 = [&#39;temperature&#39;, &#39;heart_rate&#39;,&#39;respiratory_rate&#39;,&#39;O2_saturation&#39;,&#39;systolic_blood_pressure&#39;, &#39;shock_index&#39;,&#39;diastolic_blood_pressure&#39;, &#39;pulse_pressure&#39;,&#39;mean_arterial_pressure&#39;,&#39;urine_output&#39;, &#39;serum_glucose&#39;, &#39;serum_lactate&#39;, &#39;arterial_blood_gas_lactate&#39;, &#39;arterial_blood_gas_PCO2&#39;, &#39;arterial_blood_gas_PaO2&#39;, &#39;arterial_blood_gas_pH&#39;, &#39;venous_blood_gas_lactate&#39;, &#39;venous_blood_gas_PCO2&#39;, &#39;venous_blood_gas_PaO2&#39;, &#39;venous_blood_gas_pH&#39;] feats_72 = [&#39;temperature&#39;, &#39;heart_rate&#39;,&#39;respiratory_rate&#39;,&#39;O2_saturation&#39;,&#39;systolic_blood_pressure&#39;, &#39;shock_index&#39;,&#39;diastolic_blood_pressure&#39;, &#39;pulse_pressure&#39;, &#39;mean_arterial_pressure&#39;,&#39;urine_output&#39;,&#39;serum_white_blood_count&#39;, &#39;serum_lymphocyte_count&#39;,&#39;serum_immature_granulocytes&#39;,&#39;serum_eosinophil_count&#39;,&#39;serum_monocyte_count&#39;, &#39;serum_neutrophil_count&#39;,&#39;serum_hemoglobin&#39;, &#39;serum_hematocrit&#39;, &#39;serum_platelet_count&#39;, &#39;serum_sodium&#39;, &#39;serum_chloride&#39;, &#39;serum_CO2&#39;, &#39;serum_BUN&#39;, &#39;serum_creatinine&#39;, &#39;BUN_CR&#39;, &#39;serum_glucose&#39;, &#39;serum_anion_gap&#39;, &#39;serum_bilirubin_total&#39;, &#39;serum_AST&#39;, &#39;serum_ALT&#39;, &#39;serum_ALP&#39;, &#39;serum_protein&#39;, &#39;serum_albumin&#39;, &#39;serum_lactate&#39;, &#39;arterial_blood_gas_lactate&#39;, &#39;arterial_blood_gas_PCO2&#39;, &#39;arterial_blood_gas_PaO2&#39;, &#39;arterial_blood_gas_pH&#39;, &#39;venous_blood_gas_lactate&#39;, &#39;venous_blood_gas_PCO2&#39;, &#39;venous_blood_gas_PaO2&#39;, &#39;venous_blood_gas_pH&#39;] feats_120 = [&#39;serum_white_blood_count&#39;,&#39;serum_lymphocyte_count&#39;,&#39;serum_immature_granulocytes&#39;,&#39;serum_eosinophil_count&#39;, &#39;serum_monocyte_count&#39;,&#39;serum_neutrophil_count&#39;,&#39;serum_hemoglobin&#39;, &#39;serum_hematocrit&#39;, &#39;serum_platelet_count&#39;, &#39;serum_sodium&#39;, &#39;serum_chloride&#39;, &#39;serum_CO2&#39;, &#39;serum_BUN&#39;, &#39;serum_creatinine&#39;, &#39;BUN_CR&#39;, &#39;serum_anion_gap&#39;, &#39;serum_bilirubin_total&#39;, &#39;serum_AST&#39;, &#39;serum_ALT&#39;, &#39;serum_ALP&#39;, &#39;serum_protein&#39;, &#39;serum_albumin&#39;] . min_48 = HourWindowFeat(hours=48, feats=feats_48, stat=min) min_72 = HourWindowFeat(hours=72, feats=feats_72, stat=min) min_120 = HourWindowFeat(hours=120, feats=feats_120, stat=min) max_48 = HourWindowFeat(hours=48, feats=feats_48, stat=max) max_72 = HourWindowFeat(hours=72, feats=feats_72, stat=max) max_120 = HourWindowFeat(hours=120, feats=feats_120, stat=max) mean_48 = HourWindowFeat(hours=48, feats=feats_48, stat=mean) mean_72 = HourWindowFeat(hours=72, feats=feats_72, stat=mean) mean_120 = HourWindowFeat(hours=120, feats=feats_120, stat=mean) median_48 = HourWindowFeat(hours=48, feats=feats_48, stat=&#39;median&#39;) median_72 = HourWindowFeat(hours=72, feats=feats_72, stat=&#39;median&#39;) median_120 = HourWindowFeat(hours=120, feats=feats_120, stat=&#39;median&#39;) slope_72 = HourWindowFeat(hours=72, feats=feats_72, stat=&#39;slope&#39;) slope_120 = HourWindowFeat(hours=120, feats=feats_120, stat=&#39;slope&#39;) std_24 = HourWindowFeat(hours=24, feats=feats_24, stat=stddev) std_48 = HourWindowFeat(hours=48, feats=feats_48, stat=stddev) std_72 = HourWindowFeat(hours=72, feats=feats_72, stat=stddev) std_120 = HourWindowFeat(hours=120, feats=feats_120, stat=stddev) FeaturesPipeline = Pipeline(stages=[min_48, min_72, min_120, max_48, max_72, max_120, mean_48, mean_72, mean_120, median_48, median_72, median_120, slope_72, slope_120, std_24, std_48, std_72, std_120]) FeaturesPipeline = Pipeline(stages=[min_48, min_72, min_120]) FeaturesPipeline = Pipeline(stages=[min_48, min_72, min_120, max_48, max_72, max_120]) . Finally, we fit our pipeline to the data. . FeaturesPipeline = Pipeline(stages=[min_48, min_72]) Featpip = FeaturesPipeline.fit(df) df_feats = Featpip.transform(df) . I&#39;ll be fleshing out this post when I have more time, but please feel free to send me questions/comments on anything related to this! .",
            "url": "https://zs-barnes.github.io/blog/pyspark/time%20series/2020/08/07/Pyspark-Feature-Engineering.html",
            "relUrl": "/pyspark/time%20series/2020/08/07/Pyspark-Feature-Engineering.html",
            "date": " • Aug 7, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://zs-barnes.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://zs-barnes.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a data enthusiast and programmer. Things I like: Lisp, Vim, and Python. . I’m also confused about how to split up my digital presence. Here is my music website: https://zacharybarnes.org .",
          "url": "https://zs-barnes.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "Early Prediction of Hospital Acquired Sepsis . In a collaboration with the UCSF School of Medicine, our research is on the development of a model for the early prediction of hospital acquired sepsis. Several features differentiate this research from current sepsis models in research and production: training a model using the UCSF information commons database, extensive feature engineering, labeling patients based on the most reliable definition of sepsis, and focusing on a non-ICU ward. We achieved state of the art results with an AUCPR of 0.67 four hours before onset using the leading model, a multi-task gaussian process and recurrent neural network, and comparable results with tree-based models trained with expert engineered features. . Math to Code . Web application to convert pictures of math to functioning Python code. — .",
          "url": "https://zs-barnes.github.io/blog/featured/",
          "relUrl": "/featured/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://zs-barnes.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}